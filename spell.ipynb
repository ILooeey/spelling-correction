{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YPSXHpbd1XM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b858e01-b966-485a-b0f1-1af69dcd20b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.11/dist-packages (0.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F4CJhT7XObkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4781082b-0ed6-4dca-a925-1232dd634d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `pip install transformers torchyspellchecker in /usr/local/lib/python3.11/dist-packages (0.8.2)'\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torchyspellchecker in /usr/local/lib/python3.11/dist-packages (0.8.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "import difflib\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Hapus tanda baca kecuali spasi, huruf, angka\n",
        "    text = re.sub(r'\\s+', ' ', text)     # Ganti multiple spaces jadi satu spasi\n",
        "    return text.lower().strip()\n",
        "\n",
        "# Dekomp suffix (e.g., sinyalnya -> sinyal + nya)\n",
        "def decompose_suffix(word):\n",
        "    suffixes = ['ku', 'mu', 'nya', 'lah', 'kah', 'tah', 'pun']\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix) and len(word) > len(suffix) + 1:\n",
        "            return word[:-len(suffix)], suffix\n",
        "    return word, ''\n",
        "\n",
        "# Load models and resources\n",
        "def load_resources():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "    model = AutoModelForMaskedLM.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "\n",
        "    def load_words(filepath):\n",
        "        with open(filepath, 'r', encoding='utf-8') as file:\n",
        "            words = file.read().lower().splitlines()\n",
        "        return set(words)\n",
        "\n",
        "    base_words = load_words('wordlist.txt')\n",
        "    word_list = set(base_words)\n",
        "    for word in base_words:\n",
        "        for suf in ['ku', 'mu', 'nya']:\n",
        "            word_list.add(word + suf)\n",
        "\n",
        "    df = pd.read_csv(\"hf://datasets/theonlydo/indonesia-slang/slang-indo.csv\")\n",
        "    slang_map = defaultdict(list)\n",
        "    for _, row in df.iterrows():\n",
        "        slang = str(row['slang']).lower().strip()\n",
        "        correction = str(row['formal']).lower().strip()\n",
        "        if slang and correction:\n",
        "            slang_map[slang].append(correction)\n",
        "\n",
        "    return tokenizer, model, word_list, slang_map\n",
        "\n",
        "# Load once\n",
        "tokenizer, model, word_list, slang_map = load_resources()\n",
        "\n",
        "# Ambil kandidat dengan edit distance\n",
        "def get_candidates(word, word_list, n=5, cutoff=0.6):\n",
        "    if word in word_list:\n",
        "        return [word]\n",
        "    candidates = difflib.get_close_matches(word, word_list, n=n, cutoff=cutoff)\n",
        "    if not candidates and cutoff > 0.5:\n",
        "        candidates = difflib.get_close_matches(word, word_list, n=n, cutoff=cutoff - 0.1)\n",
        "    if not candidates and len(word) >= 4:\n",
        "        candidates = difflib.get_close_matches(word, word_list, n=n, cutoff=0.4)\n",
        "    return candidates if candidates else [word]\n",
        "\n",
        "# Highlight difference\n",
        "def highlight_diff(original, corrected):\n",
        "    orig_words = original.split()\n",
        "    corr_words = corrected.split()\n",
        "    matcher = difflib.SequenceMatcher(None, orig_words, corr_words)\n",
        "    result = []\n",
        "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
        "        if tag == 'equal':\n",
        "            result.extend(corr_words[j1:j2])\n",
        "        elif tag in ['replace', 'delete', 'insert']:\n",
        "            replaced = ' '.join(corr_words[j1:j2])\n",
        "            result.append(f\"[{replaced}]\")\n",
        "    return ' '.join(result)\n",
        "\n",
        "# Correction pipeline\n",
        "def correct_text_pipeline(text, top_k=5):\n",
        "    original_text = text.strip()\n",
        "    text = preprocess_text(original_text)\n",
        "    words = text.split()\n",
        "    corrected_words = []\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        # Step 1: Slang\n",
        "        if word in slang_map:\n",
        "            corrected_words.append(slang_map[word][0])\n",
        "            continue\n",
        "\n",
        "        # Step 2: Valid word\n",
        "        if word in word_list:\n",
        "            corrected_words.append(word)\n",
        "            continue\n",
        "\n",
        "        # Step 2.5: Decompose suffix and re-check\n",
        "        base_word, suffix = decompose_suffix(word)\n",
        "        if base_word in word_list:\n",
        "            corrected_words.append(base_word + suffix)\n",
        "            continue\n",
        "\n",
        "        # Step 3: Candidates\n",
        "        candidates = get_candidates(word, word_list, n=top_k)\n",
        "        if len(candidates) == 1:\n",
        "            corrected_words.append(candidates[0])\n",
        "            continue\n",
        "\n",
        "        # Step 4: Masked Language Model\n",
        "        temp = words.copy()\n",
        "        temp[i] = tokenizer.mask_token\n",
        "        masked_text = ' '.join(temp)\n",
        "        inputs = tokenizer(masked_text, return_tensors='pt')\n",
        "        mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
        "\n",
        "        if mask_token_index.size(0) > 0:\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                predictions = outputs.logits[0, mask_token_index[0]]\n",
        "\n",
        "            candidate_scores = []\n",
        "            for candidate in candidates:\n",
        "                candidate_ids = tokenizer.encode(candidate, add_special_tokens=False)\n",
        "                if candidate_ids:\n",
        "                    score = sum(predictions[token_id].item() for token_id in candidate_ids) / len(candidate_ids)\n",
        "                    candidate_scores.append((candidate, score))\n",
        "\n",
        "            if candidate_scores:\n",
        "                candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "                corrected_words.append(candidate_scores[0][0])\n",
        "                continue\n",
        "\n",
        "        corrected_words.append(candidates[0])\n",
        "\n",
        "    return ' '.join(corrected_words)\n",
        "\n",
        "# Interactive CLI\n",
        "def interactive_correction():\n",
        "    print(\"Indonesian Text Correction System\")\n",
        "    print(\"Type 'stop' to exit\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"Masukkan kalimat: \").strip()\n",
        "        if user_input.lower() in ['stop', 'exit', 'quit']:\n",
        "            break\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        corrected = correct_text_pipeline(user_input)\n",
        "        print(\"\\nOriginal:\", user_input)\n",
        "        print(\"Corrected:\", corrected)\n",
        "        print(\"Highlight:\", highlight_diff(user_input, corrected))\n",
        "        print()\n",
        "\n",
        "# Run\n",
        "if __name__ == \"__main__\":\n",
        "    interactive_correction()\n"
      ],
      "metadata": {
        "id": "SJfHEsRFEhU0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}