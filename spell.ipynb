{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPSXHpbd1XM1"
      },
      "outputs": [],
      "source": [
        "# !pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4CJhT7XObkV"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers torchyspellchecker in /usr/local/lib/python3.11/dist-packages (0.8.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade sqlalchemy\n",
        "\n",
        "# !pip install transformers dataset\n",
        "# # !pip install transformers torch\n",
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCDM-va0Y6Ea",
        "outputId": "6a58dd58-6322-4266-edff-b18c5c7bc706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJfHEsRFEhU0",
        "outputId": "731ff17a-a302-4abc-caad-cc3f8ef13d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForMaskedLM were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indonesian Text Correction System\n",
            "Type 'stop' to exit\n",
            "\n",
            "Masukkan kalimat: kmu jgn jalan kejauhan\n",
            "\n",
            "Original: kmu jgn jalan kejauhan\n",
            "Corrected: kamu jangan jalan kejauhan\n",
            "Highlight: [kamu jangan] jalan kejauhan\n",
            "\n",
            "Masukkan kalimat: stop\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "import difflib\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Hapus tanda baca\n",
        "    text = re.sub(r'\\s+', ' ', text)     # Ganti jadi satu spasi\n",
        "    return text.lower().strip()\n",
        "\n",
        "# Dekomp suffix (e.g., sinyalnya -> sinyal + nya)\n",
        "def decompose_suffix(word):\n",
        "    suffixes = ['ku', 'mu', 'nya', 'lah', 'kah', 'tah', 'pun']\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix) and len(word) > len(suffix) + 1:\n",
        "            base = word[:-len(suffix)] #menghapus suffix\n",
        "            if base in word_list:\n",
        "                return base, suffix\n",
        "    return word, ''\n",
        "\n",
        "# Load models and resources\n",
        "def load_resources():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "    model = AutoModelForMaskedLM.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "\n",
        "    def load_words(filepath):\n",
        "        with open(filepath, 'r', encoding='utf-8') as file:\n",
        "            words = file.read().lower().splitlines()\n",
        "        return set(words)\n",
        "\n",
        "    base_words = load_words('wordlist.txt')\n",
        "    word_list = set(base_words)\n",
        "    for word in base_words:\n",
        "        for suf in ['ku', 'mu', 'nya']:\n",
        "            word_list.add(word + suf)\n",
        "\n",
        "    df = pd.read_csv(\"hf://datasets/theonlydo/indonesia-slang/slang-indo.csv\")\n",
        "    slang_map = defaultdict(list)\n",
        "    for _, row in df.iterrows():\n",
        "        slang = str(row['slang']).lower().strip()\n",
        "        correction = str(row['formal']).lower().strip()\n",
        "        if slang and correction:\n",
        "            slang_map[slang].append(correction)\n",
        "\n",
        "    return tokenizer, model, word_list, slang_map\n",
        "\n",
        "# Load once\n",
        "tokenizer, model, word_list, slang_map = load_resources()\n",
        "\n",
        "# Ambil kandidat dengan edit distance\n",
        "def get_candidates(word, word_list, n=5, cutoff=0.6):\n",
        "    if word in word_list:\n",
        "        return [word]\n",
        "    candidates = difflib.get_close_matches(word, word_list, n=n, cutoff=cutoff)  #mencari kemiripan\n",
        "    if not candidates and cutoff > 0.5:\n",
        "        candidates = difflib.get_close_matches(word, word_list, n=n, cutoff=cutoff - 0.1)\n",
        "    if not candidates and len(word) >= 4:\n",
        "        candidates = difflib.get_close_matches(word, word_list, n=n, cutoff=0.4)\n",
        "    return candidates if candidates else [word]\n",
        "\n",
        "# Highlight difference\n",
        "def highlight_diff(original, corrected):\n",
        "    orig_words = original.split()\n",
        "    corr_words = corrected.split()\n",
        "    matcher = difflib.SequenceMatcher(None, orig_words, corr_words)\n",
        "    result = []\n",
        "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
        "        if tag == 'equal': #setara\n",
        "            result.extend(corr_words[j1:j2])\n",
        "        elif tag in ['replace', 'delete', 'insert']:\n",
        "            replaced = ' '.join(corr_words[j1:j2])\n",
        "            result.append(f\"[{replaced}]\")\n",
        "    return ' '.join(result)\n",
        "\n",
        "# Correction pipeline\n",
        "def correct_text_pipeline(text, top_k=5):\n",
        "    original_text = text.strip()\n",
        "    text = preprocess_text(original_text)\n",
        "    words = text.split()\n",
        "    corrected_words = []\n",
        "\n",
        "#perulangan i dan word pada list\n",
        "    for i, word in enumerate(words):\n",
        "        # Step 1: Slang\n",
        "        if word in slang_map:\n",
        "            corrected_words.append(slang_map[word][0])\n",
        "            continue\n",
        "\n",
        "        # Step 2: Valid word\n",
        "        if word in word_list:\n",
        "            corrected_words.append(word)\n",
        "            continue\n",
        "\n",
        "        # Step 2.5: Decompose suffix and re-check\n",
        "        base_word, suffix = decompose_suffix(word)\n",
        "        if base_word in word_list:\n",
        "            corrected_words.append(base_word + suffix)\n",
        "            continue\n",
        "\n",
        "        # Step 3: Candidates\n",
        "        candidates = get_candidates(word, word_list, n=top_k)\n",
        "        if len(candidates) == 1:\n",
        "            corrected_words.append(candidates[0])\n",
        "            continue\n",
        "\n",
        "        # Step 4: Masked Language Model\n",
        "        temp = words.copy()\n",
        "        temp[i] = tokenizer.mask_token\n",
        "        masked_text = ' '.join(temp)\n",
        "        inputs = tokenizer(masked_text, return_tensors='pt')\n",
        "        mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1] #Menemukan posisi index dalam output tokenisasi\n",
        "\n",
        "        if mask_token_index.size(0) > 0: # Memastikan bahwa ada token [MASK]\n",
        "            with torch.no_grad(): # Menjalankan model tanpa menyimpan informasi.\n",
        "                outputs = model(**inputs)\n",
        "                predictions = outputs.logits[0, mask_token_index[0]]\n",
        "\n",
        "            candidate_scores = []\n",
        "            for candidate in candidates:\n",
        "                candidate_ids = tokenizer.encode(candidate, add_special_tokens=False)\n",
        "                if candidate_ids:\n",
        "                    score = sum(predictions[token_id].item() for token_id in candidate_ids) / len(candidate_ids)\n",
        "                    candidate_scores.append((candidate, score))\n",
        "\n",
        "            if candidate_scores:\n",
        "              candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "              print(f\"\\n[Info] Kata salah: '{word}' â†’ Kandidat MLM:\")\n",
        "              for cand, score in candidate_scores:\n",
        "                  print(f\"  - {cand:<15} | skor: {score:.4f}\")\n",
        "\n",
        "              corrected_words.append(candidate_scores[0][0])\n",
        "              continue\n",
        "\n",
        "\n",
        "        corrected_words.append(candidates[0])\n",
        "\n",
        "    return ' '.join(corrected_words)\n",
        "\n",
        "# Interactive CLI\n",
        "def interactive_correction():\n",
        "    print(\"Indonesian Text Correction System\")\n",
        "    print(\"Type 'stop' to exit\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"Masukkan kalimat: \").strip()\n",
        "        if user_input.lower() in ['stop', 'exit', 'quit']:\n",
        "            break\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        corrected = correct_text_pipeline(user_input)\n",
        "        print(\"\\nOriginal:\", user_input)\n",
        "        print(\"Corrected:\", corrected)\n",
        "        print(\"Highlight:\", highlight_diff(user_input, corrected))\n",
        "        print()\n",
        "\n",
        "# Run\n",
        "if __name__ == \"__main__\":\n",
        "    interactive_correction()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}